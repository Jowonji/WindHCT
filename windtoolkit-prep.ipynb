{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be40874",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We also referenced **WiSoSuper** (Caltech SURF 2020–2021), a public benchmark study of super-resolution models on wind and solar fields built from NREL WIND Toolkit and NSRDB data.\n",
    "\n",
    "In this repository, we follow WiSoSuper mainly for its HSDS-based data access patterns and dataset construction workflow (e.g., cropping and tiling), while using our own choices for variables, downsampling, and storage format.\n",
    "\n",
    "Reference: https://github.com/RupaKurinchiVendhan/WiSoSuper/blob/main/data.ipynb\n",
    "\n",
    "### Key Differences from the Reference Implementation\n",
    "See the dataset README for details: [datasets/README.md](datasets/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858d55b",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "\n",
    "Install dependencies (run once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q h5pyd numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33050ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "os.environ[\"NREL_API_KEY\"] = getpass.getpass(\"Enter NREL API key (hidden): \")\n",
    "print(\"Key loaded:\", \"NREL_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91d0f3",
   "metadata": {},
   "source": [
    "Set your NREL API key via an environment variable (recommended for public repos):\n",
    "\n",
    "- **macOS/Linux**\n",
    "  ```bash\n",
    "  export NREL_API_KEY=\"YOUR_KEY\"\n",
    "\n",
    "- **Windows (PowerShell)**\n",
    "\n",
    "  ```powershell\n",
    "  setx NREL_API_KEY \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5350170",
   "metadata": {},
   "source": [
    "## 2) Configuration\n",
    "\n",
    "Edit `CONFIG` below to match your needs. Key choices to document in a public repo:\n",
    "\n",
    "- `row_slice` / `col_slice`: what region you select and why\n",
    "- `sampling_interval`: your intended temporal cadence\n",
    "- filters (`min_mean`, `min_value_gt`): these can bias the distribution (explain or simplify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import h5pyd\n",
    "\n",
    "CONFIG = {\n",
    "    \"h5_path\": \"/nrel/wtk-us.h5\",\n",
    "    \"bucket\": \"nrel-pds-hsds\",\n",
    "    \"endpoint\": \"https://developer.nrel.gov/api/hsds\",\n",
    "    \"api_key_env\": \"NREL_API_KEY\",\n",
    "    \"speed_dataset\": \"windspeed_100m\",\n",
    "\n",
    "    \"row_slice\": (0, 1600),\n",
    "    \"col_slice\": (500, 2100),\n",
    "\n",
    "    \"sampling_interval\": 48,\n",
    "\n",
    "    \"tile_hr\": (100, 100),\n",
    "    \"down_factor\": 5,\n",
    "    \"tile_keep_ratio\": 0.30,\n",
    "\n",
    "    \"drop_if_nan\": True,\n",
    "    \"min_value_gt\": 0.0,\n",
    "    \"min_mean\": 0.5,\n",
    "\n",
    "    \"train_ratio\": 0.70,\n",
    "    \"val_ratio\": 0.15,\n",
    "\n",
    "    \"max_samples\": 40000,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    \"out_dir\": \"./dataset_wind_sr\",\n",
    "    \"chunk_flush\": 2048,\n",
    "}\n",
    "\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d86e46",
   "metadata": {},
   "source": [
    "## 3) Utilities\n",
    "\n",
    "This section defines helper functions for:\n",
    "- average pooling downsampling (HR → LR)\n",
    "- time-ordered timestep splitting (to reduce temporal leakage)\n",
    "- safe HSDS file opening (API key via env var)\n",
    "- streaming writes via NumPy memmap (memory-efficient dataset generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfebb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def avg_pool2d(arr: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Block average pooling for a 2D array (H, W) with kernel size k.\n",
    "    Requires H and W to be divisible by k.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    if (h % k != 0) or (w % k != 0):\n",
    "        raise ValueError(f\"Input shape {arr.shape} must be divisible by k={k}.\")\n",
    "    return arr.reshape(h // k, k, w // k, k).mean(axis=(1, 3))\n",
    "\n",
    "def open_hsds_file(cfg: dict):\n",
    "    \"\"\"\n",
    "    Open remote HSDS file using an API key from an env var (do NOT hardcode keys).\n",
    "    \"\"\"\n",
    "    api_key = os.environ.get(cfg[\"api_key_env\"], \"\").strip()\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\n",
    "            f\"Missing API key. Please set environment variable {cfg['api_key_env']}.\"\n",
    "        )\n",
    "\n",
    "    f = h5pyd.File(\n",
    "        cfg[\"h5_path\"],\n",
    "        \"r\",\n",
    "        bucket=cfg[\"bucket\"],\n",
    "        endpoint=cfg[\"endpoint\"],\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    return f\n",
    "\n",
    "class MemmapWriter:\n",
    "    \"\"\"\n",
    "    Stream samples to disk using numpy memmap to avoid holding everything in RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, shape: tuple, dtype=np.float32):\n",
    "        self.path = path\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "        self.mm = np.memmap(path, dtype=dtype, mode=\"w+\", shape=shape)\n",
    "\n",
    "    def write(self, idx: int, arr: np.ndarray):\n",
    "        self.mm[idx] = arr\n",
    "\n",
    "    def flush(self):\n",
    "        self.mm.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.flush()\n",
    "        del self.mm  # ensure file is closed\n",
    "\n",
    "def finalize_npy_from_memmap(dat_path: str, out_npy_path: str, final_shape: tuple, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Convert a raw memmap .dat file into a standard .npy file with exact final shape.\n",
    "    Note: this loads `final_shape` into RAM once. If too big, prefer zarr/npz chunks.\n",
    "    \"\"\"\n",
    "    mm = np.memmap(dat_path, dtype=dtype, mode=\"r\", shape=final_shape)\n",
    "    arr = np.asarray(mm)\n",
    "    np.save(out_npy_path, arr)\n",
    "    del mm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fbe581",
   "metadata": {},
   "source": [
    "## 4) Access HSDS and Inspect the Dataset\n",
    "\n",
    "We connect to the NREL HSDS endpoint and verify the dataset shape and dtype.\n",
    "Expected shape is `(time, y, x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2eed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open_hsds_file(CONFIG)\n",
    "dset_speed = f[CONFIG[\"speed_dataset\"]]\n",
    "\n",
    "print(\"Dataset:\", CONFIG[\"speed_dataset\"])\n",
    "print(\"Shape  :\", dset_speed.shape)   # (time, y, x)\n",
    "print(\"Dtype  :\", dset_speed.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c79bf",
   "metadata": {},
   "source": [
    "## 5) Tile Extraction + Downsampling (per split)\n",
    "\n",
    "For each timestep:\n",
    "1. crop a fixed spatial region (`row_slice`, `col_slice`)\n",
    "2. split into HR tiles (default `100×100`)\n",
    "3. keep a random subset of tiles (`tile_keep_ratio`)\n",
    "4. apply minimal filtering\n",
    "5. generate LR tiles via **block average pooling**\n",
    "6. stream-save tiles to disk using memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_tiles(\n",
    "    dset_speed,\n",
    "    cfg: dict,\n",
    "    rng: np.random.Generator,\n",
    "):\n",
    "    tile_h, tile_w = cfg[\"tile_hr\"]\n",
    "    k = cfg[\"down_factor\"]\n",
    "    tile_lr_h, tile_lr_w = tile_h // k, tile_w // k\n",
    "\n",
    "    r0, r1 = cfg[\"row_slice\"]\n",
    "    c0, c1 = cfg[\"col_slice\"]\n",
    "\n",
    "    total_timesteps = dset_speed.shape[0]\n",
    "    valid_timesteps = np.arange(0, total_timesteps, cfg[\"sampling_interval\"], dtype=np.int64)\n",
    "\n",
    "    HR_tiles = []\n",
    "    LR_tiles = []\n",
    "\n",
    "    for ts in valid_timesteps:\n",
    "        speed = dset_speed[int(ts), r0:r1, c0:c1]\n",
    "        speed = np.asarray(speed, dtype=np.float32)\n",
    "\n",
    "        n_h = speed.shape[0] // tile_h\n",
    "        n_w = speed.shape[1] // tile_w\n",
    "        if n_h <= 0 or n_w <= 0:\n",
    "            continue\n",
    "\n",
    "        tile_indices = [(i, j) for i in range(n_h) for j in range(n_w)]\n",
    "        keep_n = max(1, int(len(tile_indices) * cfg[\"tile_keep_ratio\"]))\n",
    "        chosen = rng.choice(len(tile_indices), size=keep_n, replace=False)\n",
    "\n",
    "        for idx in chosen:\n",
    "            i, j = tile_indices[int(idx)]\n",
    "            tile_hr = speed[i*tile_h:(i+1)*tile_h, j*tile_w:(j+1)*tile_w]\n",
    "\n",
    "            # Filters (same as original)\n",
    "            if cfg[\"drop_if_nan\"] and np.isnan(tile_hr).any():\n",
    "                continue\n",
    "            if np.min(tile_hr) <= cfg[\"min_value_gt\"]:\n",
    "                continue\n",
    "            if np.mean(tile_hr) < cfg[\"min_mean\"]:\n",
    "                continue\n",
    "\n",
    "            tile_lr = avg_pool2d(tile_hr, k).astype(np.float32)\n",
    "\n",
    "            HR_tiles.append(tile_hr.astype(np.float32))\n",
    "            LR_tiles.append(tile_lr)\n",
    "\n",
    "            if len(HR_tiles) >= cfg[\"max_samples\"]:\n",
    "                print(\"Reached max_samples.\")\n",
    "                return np.array(HR_tiles), np.array(LR_tiles)\n",
    "\n",
    "        print(f\"Timestep {int(ts)} processed. Total tiles: {len(HR_tiles)}\")\n",
    "\n",
    "    return np.array(HR_tiles), np.array(LR_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ba6e7",
   "metadata": {},
   "source": [
    "## 6) Generate Dataset (random tile-level split)\n",
    "\n",
    "All HR/LR tiles are first collected across timesteps.\n",
    "The dataset is then split into train/validation/test sets\n",
    "using random shuffling at the tile level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(CONFIG[\"seed\"])\n",
    "\n",
    "ensure_dir(CONFIG[\"out_dir\"])\n",
    "\n",
    "# Build all tiles\n",
    "HR_all, LR_all = build_all_tiles(dset_speed, CONFIG, rng)\n",
    "\n",
    "print(\"Total tiles collected:\", HR_all.shape[0])\n",
    "\n",
    "# Random tile-level split (original implementation)\n",
    "N = HR_all.shape[0]\n",
    "indices = rng.permutation(N)\n",
    "\n",
    "train_end = int(CONFIG[\"train_ratio\"] * N)\n",
    "val_end = train_end + int(CONFIG[\"val_ratio\"] * N)\n",
    "\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "HR_train, HR_val, HR_test = HR_all[train_idx], HR_all[val_idx], HR_all[test_idx]\n",
    "LR_train, LR_val, LR_test = LR_all[train_idx], LR_all[val_idx], LR_all[test_idx]\n",
    "\n",
    "# Save\n",
    "np.save(os.path.join(CONFIG[\"out_dir\"], \"train_HR.npy\"), HR_train)\n",
    "np.save(os.path.join(CONFIG[\"out_dir\"], \"train_LR.npy\"), LR_train)\n",
    "\n",
    "np.save(os.path.join(CONFIG[\"out_dir\"], \"val_HR.npy\"), HR_val)\n",
    "np.save(os.path.join(CONFIG[\"out_dir\"], \"val_LR.npy\"), LR_val)\n",
    "\n",
    "np.save(os.path.join(CONFIG[\"out_dir\"], \"test_HR.npy\"), HR_test)\n",
    "np.save(os.path.join(CONFIG[\"out_dir\"], \"test_LR.npy\"), LR_test)\n",
    "\n",
    "print(\"Dataset saved using random tile-level split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c95408",
   "metadata": {},
   "source": [
    "## 7) Visualize a Few HR/LR Pairs (optional)\n",
    "\n",
    "Loads `train_HR.npy` and `train_LR.npy` and plots a few random examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_hr = np.load(os.path.join(CONFIG[\"out_dir\"], \"train_HR.npy\"), mmap_mode=\"r\")\n",
    "train_lr = np.load(os.path.join(CONFIG[\"out_dir\"], \"train_LR.npy\"), mmap_mode=\"r\")\n",
    "\n",
    "n = train_hr.shape[0]\n",
    "rng_vis = np.random.default_rng(CONFIG[\"seed\"] + 123)\n",
    "idxs = rng_vis.choice(n, size=min(5, n), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(len(idxs), 2, figsize=(8, 3*len(idxs)))\n",
    "if len(idxs) == 1:\n",
    "    axes = np.array([axes])\n",
    "\n",
    "for r, idx in enumerate(idxs):\n",
    "    axes[r, 0].imshow(train_hr[idx], cmap=\"viridis\")\n",
    "    axes[r, 0].set_title(f\"HR (100x100) idx={idx}\")\n",
    "    axes[r, 0].axis(\"off\")\n",
    "\n",
    "    axes[r, 1].imshow(train_lr[idx], cmap=\"viridis\")\n",
    "    axes[r, 1].set_title(f\"LR (20x20) idx={idx}\")\n",
    "    axes[r, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Train HR shape:\", train_hr.shape)\n",
    "print(\"Train LR shape:\", train_lr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faae97a",
   "metadata": {},
   "source": [
    "## 8) Cleanup\n",
    "\n",
    "Close the HSDS file handle when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "print(\"Closed HSDS file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
